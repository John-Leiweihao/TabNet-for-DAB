{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc48c5f-088d-4ab1-9017-cf7f3b175986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "#Load data\n",
    "df=pd.read_csv('.\\data.\\processed_EPS1.csv',index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa4b2cc-ce00-4ac4-8414-60cf7602e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor,TabNetClassifier\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "import wget\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b5c6ed-658c-49c2-8ccd-bf892863dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(['D2', 'D0', 'Validity', 'ipk', 'irms', 'Vo', 'nZVS'], axis=1)\n",
    "df1 = df1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3981a3b-f16a-48c2-960d-ba148894dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Ptotal'\n",
    "if \"Set\" not in df1.columns:\n",
    "    df1[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(df1.shape[0],))\n",
    "\n",
    "train_indices = df1[df1.Set==\"train\"].index\n",
    "valid_indices = df1[df1.Set==\"valid\"].index\n",
    "test_indices = df1[df1.Set==\"test\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ac284-7512-4674-b68b-21498bdb9149",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06334c16-464b-46ac-b925-6dcf4109acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Int64Index to DataFrame\n",
    "train_indices_df = pd.DataFrame(train_indices, columns=['index'])\n",
    "\n",
    "# 20% random selection from train_indices \n",
    "new_train_indices_df1 = train_indices_df.sample(frac=0.2)   # Select 20 per cent of the data, you can adjust the value of frac\n",
    "\n",
    "# Extract index and convert back to Int64Index\n",
    "new_train_indices1 = pd.Index(new_train_indices_df1['index']).astype('int64')\n",
    "\n",
    "# Output new train_indices\n",
    "print(new_train_indices1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc2307-78ad-43f7-be0e-289050319e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40% random selection from train_indices \n",
    "new_train_indices_df2 = train_indices_df.sample(frac=0.4)  \n",
    "\n",
    "new_train_indices2 = pd.Index(new_train_indices_df2['index']).astype('int64')\n",
    "\n",
    "print(new_train_indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f8e41-6e53-4998-98a2-5016fddfdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% random selection from train_indices \n",
    "new_train_indices_df3 = train_indices_df.sample(frac=0.6) \n",
    "\n",
    "new_train_indices3 = pd.Index(new_train_indices_df3['index']).astype('int64')\n",
    "\n",
    "print(new_train_indices3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ac57f-7444-44b0-ace5-d2ecf48ae129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% random selection from train_indices \n",
    "new_train_indices_df4 = train_indices_df.sample(frac=0.8) \n",
    "\n",
    "new_train_indices4 = pd.Index(new_train_indices_df4['index']).astype('int64')\n",
    "\n",
    "print(new_train_indices4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f759a36-28d8-4240-a875-2e848fe6803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TabNetRegressor(n_d=16,n_a=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f444364-7e49-4302-a65d-a91138bc874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_feat = ['Set']\n",
    "features = [ col for col in df1.columns if col not in unused_feat+[target]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ea29b-ebfe-406f-8162-8487a1fcaaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df1[features].values[train_indices]\n",
    "y_train = df1[target].values[train_indices].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddcb44-264d-459a-8aef-e5c474c264b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = df1[features].values[new_train_indices1]\n",
    "y_train1 = df1[target].values[new_train_indices1].reshape(-1, 1)\n",
    "\n",
    "X_valid = df1[features].values[valid_indices]\n",
    "y_valid = df1[target].values[valid_indices].reshape(-1, 1)\n",
    "\n",
    "X_test = df1[features].values[test_indices]\n",
    "y_test = df1[target].values[test_indices].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c7b45-e5db-4e8a-a163-08fba8c70f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 400 if not os.getenv(\"CI\", False) else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5b494-2a5b-4f03-afbc-3ef82e66d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train1, y_train=y_train1,\n",
    "    eval_set=[(X_train1, y_train1), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['mse'],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=50,\n",
    "    batch_size=2048, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    #augmentations=aug\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59570344-56f2-44f0-8431-3de401191a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)\n",
    "\n",
    "y_true = y_test\n",
    "\n",
    "test_score = mean_squared_error(y_pred=preds, y_true=y_true)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21bd464-5c38-4afa-b6ac-b2fe18a27208",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = df1[features].values[new_train_indices2]\n",
    "y_train2 = df1[target].values[new_train_indices2].reshape(-1, 1)\n",
    "\n",
    "X_valid = df1[features].values[valid_indices]\n",
    "y_valid = df1[target].values[valid_indices].reshape(-1, 1)\n",
    "\n",
    "X_test = df1[features].values[test_indices]\n",
    "y_test = df1[target].values[test_indices].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8e76a-ab18-4a5c-83b3-7df9bca5ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train2, y_train=y_train2,\n",
    "    eval_set=[(X_train2, y_train2), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['mse'],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=50,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    #augmentations=aug\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b4f12-4393-49e2-baa7-4ff117a5ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)\n",
    "\n",
    "y_true = y_test\n",
    "\n",
    "test_score = mean_squared_error(y_pred=preds, y_true=y_true)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f11a7-5024-475e-8d13-d48189fdd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = df1[features].values[new_train_indices3]\n",
    "y_train3 = df1[target].values[new_train_indices3].reshape(-1, 1)\n",
    "\n",
    "X_valid = df1[features].values[valid_indices]\n",
    "y_valid = df1[target].values[valid_indices].reshape(-1, 1)\n",
    "\n",
    "X_test = df1[features].values[test_indices]\n",
    "y_test = df1[target].values[test_indices].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca15e61-e3fb-4955-89a1-f86066d2e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train3, y_train=y_train3,\n",
    "    eval_set=[(X_train3, y_train3), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['mse'],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=50,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    #augmentations=aug\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d32ea-25cc-4f87-9098-04e57f6c4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)\n",
    "\n",
    "y_true = y_test\n",
    "\n",
    "test_score = mean_squared_error(y_pred=preds, y_true=y_true)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec5d97-f108-4642-ae4a-4e6df5739d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train4 = df1[features].values[new_train_indices4]\n",
    "y_train4 = df1[target].values[new_train_indices4].reshape(-1, 1)\n",
    "\n",
    "X_valid = df1[features].values[valid_indices]\n",
    "y_valid = df1[target].values[valid_indices].reshape(-1, 1)\n",
    "\n",
    "X_test = df1[features].values[test_indices]\n",
    "y_test = df1[target].values[test_indices].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1826e3b7-a088-49d5-aa8e-00ca5dff4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train4, y_train=y_train4,\n",
    "    eval_set=[(X_train4, y_train4), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['mse'],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=50,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    #augmentations=aug\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98763a7d-4369-4ef7-8464-a3cf3b39d9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " preds = clf.predict(X_test)\n",
    "\n",
    "y_true = y_test\n",
    "\n",
    "test_score = mean_squared_error(y_pred=preds, y_true=y_true)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088bc60-1541-4ceb-be62-ce75dbf2c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['mse'],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=300,\n",
    "    batch_size=512, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    #augmentations=aug\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ccaec8-953d-4187-b372-c969453b76d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)\n",
    "\n",
    "y_true = y_test\n",
    "\n",
    "test_score = mean_squared_error(y_pred=preds, y_true=y_true)\n",
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c8120-e83a-4075-8734-7f599cdf0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix, masks = clf.explain(X_test)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
    "\n",
    "for i in range(3):\n",
    "    axs[i].imshow(masks[i][:30])\n",
    "    axs[i].set_title(f\"mask {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d31460-ba72-4d65-b837-8985eb82c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22d569-3f59-4985-b6de-20f29e00974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75026700-6e99-44fb-b387-0f45e9ad077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#20% training data\n",
    "# Define the Random Forest Model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Define the Hyperparameter Grid\n",
    "param_grid = {\n",
    "    'n_estimators': [30, 50, 100],\n",
    "    'max_depth': [5, 8, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning with grid searc\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train1, y_train1)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Prediction on the test set\n",
    "Ptotal_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the MSE and MAE on the test set\n",
    "mse_test = mean_squared_error(y_test, Ptotal_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, Ptotal_test_pred)\n",
    "\n",
    "print(f\"Best Model - MSE on Test Set: {mse_test:.4f} - MAPE on Test Set: {mape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b0e49-c794-4376-971f-21f0c1153059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#40% training data\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [30, 50, 100],\n",
    "    'max_depth': [5, 8, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train2, y_train2)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "Ptotal_test_pred = best_model.predict(X_test)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, Ptotal_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, Ptotal_test_pred)\n",
    "\n",
    "print(f\"Best Model - MSE on Test Set: {mse_test:.4f} - MAPE on Test Set: {mape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51b6e5-18b9-41aa-ae39-223d8bf9db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#60% training data\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [30, 50, 100],\n",
    "    'max_depth': [5, 8, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train3, y_train3)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "Ptotal_test_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "mse_test = mean_squared_error(y_test, Ptotal_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, Ptotal_test_pred)\n",
    "\n",
    "print(f\"Best Model - MSE on Test Set: {mse_test:.4f} - MAPE on Test Set: {mape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d591cbf-01b0-4076-b5b7-ba31b315114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#80% training data\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [30, 50, 100],\n",
    "    'max_depth': [5, 8, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train4, y_train4)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "Ptotal_test_pred = best_model.predict(X_test)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, Ptotal_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, Ptotal_test_pred)\n",
    "\n",
    "print(f\"Best Model - MSE on Test Set: {mse_test:.4f} - MAPE on Test Set: {mape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c6d198-4b28-4062-817d-c381b613403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All training data\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [30, 50, 100],\n",
    "    'max_depth': [5, 8, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "Ptotal_test_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "mse_test = mean_squared_error(y_test, Ptotal_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, Ptotal_test_pred)\n",
    "\n",
    "print(f\"Best Model - MSE on Test Set: {mse_test:.4f} - MAPE on Test Set: {mape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31d3c8-0fe3-4c9b-a453-518721515eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#20% training data\n",
    "# Define the XGBoost Model\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "\n",
    "param_grid =  {\n",
    "    'n_estimators': [50,100,150],\n",
    "    'max_depth': [3,4,5],\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train1, y_train1)\n",
    "\n",
    "\n",
    "best_model1 = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "Ptotal_test_pred = best_model1.predict(X_test)\n",
    "\n",
    "\n",
    "mse_test = mean_squared_error(y_test, Ptotal_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, Ptotal_test_pred)\n",
    "\n",
    "print(f\"Best Model - MSE on Test Set: {mse_test:.4f} - MAPE on Test Set: {mape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895222bb-5a79-4090-854d-0d377473d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#40% training data\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50,100,150],\n",
    "    'max_depth': [3, 4, 5],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train2, y_train2)\n",
    "\n",
    "\n",
    "best_model1 = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "Ptotal_test_pred = best_model1.predict(X_test)\n",
    "\n",
    "\n",
    "mse_test = mean_squared_error(y_test, Ptotal_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, Ptotal_test_pred)\n",
    "\n",
    "print(f\"Best Model - MSE on Test Set: {mse_test:.4f} - MAPE on Test Set: {mape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c3161-2f98-4283-a67d-bc740eb90260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#60% training data\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100,150],\n",
    "    'max_depth': [3,4,5],\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train3, y_train3)\n",
    "\n",
    "\n",
    "best_model1 = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "Ptotal_test_pred = best_model1.predict(X_test)\n",
    "\n",
    "\n",
    "mse_test = mean_squared_error(y_test, Ptotal_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, Ptotal_test_pred)\n",
    "\n",
    "print(f\"Best Model - MSE on Test Set: {mse_test:.4f} - MAPE on Test Set: {mape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3118efd1-b124-48d7-acf6-4caf219478a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#80% training data\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50,100,150],\n",
    "    'max_depth': [3,4,5],\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train4, y_train4)\n",
    "\n",
    "\n",
    "best_model1 = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "Ptotal_test_pred = best_model1.predict(X_test)\n",
    "\n",
    "\n",
    "mse_test = mean_squared_error(y_test, Ptotal_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, Ptotal_test_pred)\n",
    "\n",
    "print(f\"Best Model - MSE on Test Set: {mse_test:.4f} - MAPE on Test Set: {mape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "abc2a204-eb88-470d-aa48-eb7d589314ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model - MSE on Test Set: 3.5182 - MAPE on Test Set: 0.0830\n"
     ]
    }
   ],
   "source": [
    "# All training data\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50,100,150],\n",
    "    'max_depth': [3,4,5],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model1 = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "Ptotal_test_pred = best_model1.predict(X_test)\n",
    "\n",
    "\n",
    "mse_test = mean_squared_error(y_test, Ptotal_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, Ptotal_test_pred)\n",
    "\n",
    "print(f\"Best Model - MSE on Test Set: {mse_test:.4f} - MAPE on Test Set: {mape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37eac3e-02c8-4844-a91f-e8c90b718828",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_inputs=MinMaxScaler()\n",
    "scaler_inputs.fit(X_train)\n",
    "inputs_train=scaler_inputs.transform(X_train)\n",
    "inputs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a559328-03c7-4aba-bdf2-fd51297ea8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_inputs=MinMaxScaler()\n",
    "scaler_inputs.fit(X_valid)\n",
    "inputs_valid=scaler_inputs.transform(X_valid)\n",
    "inputs_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e2ab9-54ce-4ea5-9c77-8e2b797d54ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_inputs=MinMaxScaler()\n",
    "scaler_inputs.fit(X_test)\n",
    "inputs_test=scaler_inputs.transform(X_test)\n",
    "inputs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92af47-d003-442f-ad1a-2f140f554842",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_inputs=MinMaxScaler()\n",
    "scaler_inputs.fit(X_train1)\n",
    "inputs_train1=scaler_inputs.transform(X_train1)\n",
    "inputs_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbeea0-cbfe-4f22-a7ac-ea19dac3a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_inputs=MinMaxScaler()\n",
    "scaler_inputs.fit(X_train2)\n",
    "inputs_train2=scaler_inputs.transform(X_train2)\n",
    "inputs_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80369e96-5e48-411b-a5cd-20e8c5e15124",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_inputs=MinMaxScaler()\n",
    "scaler_inputs.fit(X_train3)\n",
    "inputs_train3=scaler_inputs.transform(X_train3)\n",
    "inputs_train3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a18ee-99b5-4e0c-89b4-f33149e7eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_inputs=MinMaxScaler()\n",
    "scaler_inputs.fit(X_train4)\n",
    "inputs_train4=scaler_inputs.transform(X_train4)\n",
    "inputs_train4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40909bd3-8616-44c1-9a25-c9a6d147aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#20% training data\n",
    "#Define the SVR Model\n",
    "model = SVR()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1,10, 100],\n",
    "    'kernel': ['linear',  'rbf'],\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(inputs_train, y_train)\n",
    "\n",
    "\n",
    "best_model1 = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ptotal_test_pred = best_model1.predict(inputs_test)\n",
    "\n",
    "\n",
    "mse_test = mean_squared_error(y_test, Ptotal_test_pred)\n",
    "mape_test = mean_absolute_percentage_error(y_test, Ptotal_test_pred)\n",
    "\n",
    "print(f\"Best Model - MSE on Test Set: {mse_test:.4f} - MAPE on Test Set: {mape_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1b999-09fd-47cc-a6e6-23aedfa4ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math,copy,time\n",
    "from torch.autograd import Variable\n",
    "class Single_Residual(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,dropout,final=False):\n",
    "        super(Single_Residual,self).__init__()\n",
    "        self.linear_layer=nn.Linear(input_size,hidden_size)\n",
    "        self.final=final\n",
    "        if not self.final:\n",
    "            self.batch_norm=nn.BatchNorm1d(hidden_size)\n",
    "            self.dropout=nn.Dropout(dropout)\n",
    "        self.residual_maker=nn.Parameter(torch.zeros(input_size,hidden_size))\n",
    "        self.residual_maker.requires_grad_(False)\n",
    "        if hidden_size<=input_size:\n",
    "            self.residual_maker[torch.randperm(input_size)[:hidden_size],torch.arange(0,hidden_size)]=1\n",
    "        else:\n",
    "            self.residual_maker[torch.arange(0,input_size),torch.randperm(hidden_size)[:input_size]] = 1\n",
    "            \n",
    "    def forward(self,x):\n",
    "        if self.final:\n",
    "            h1=self.linear_layer(x)\n",
    "        else:\n",
    "            h1=F.relu(self.linear_layer(x))\n",
    "        return h1\n",
    "\n",
    "class Residual_DNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_sizes,dropout):\n",
    "        super(Residual_DNN,self).__init__()\n",
    "        assert isinstance(hidden_sizes,list),'the \"hidden_sizes\" should be list '\n",
    "        hidden_num=len(hidden_sizes)\n",
    "        self.residuals=[f\"self.residual{i}\"for i in range(hidden_num)]\n",
    "        for i in range(hidden_num):\n",
    "            if i==hidden_num-1:\n",
    "                exec(self.residuals[i]+\"=Single_Residual(input_size,hidden_sizes[i],dropout,True)\")\n",
    "            else:\n",
    "                exec(self.residuals[i]+\"=Single_Residual(input_size,hidden_sizes[i],dropout)\")\n",
    "            input_size=hidden_sizes[i]\n",
    "    def forward(self,x):\n",
    "        for residual in self.residuals:\n",
    "            x=eval(residual)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be6eab-d761-45b9-803b-bcd9a7363a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# 20% traning data\n",
    "# Data type conversion\n",
    "inputs_train1 = torch.tensor(inputs_train1).float()\n",
    "y_train1 = torch.tensor(y_train1).float()\n",
    "inputs_valid = torch.tensor(inputs_valid).float()\n",
    "y_valid = torch.tensor(y_valid).float()\n",
    "inputs_test = torch.tensor(inputs_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(inputs_train1, y_train1)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=1024, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "input_size = inputs_train.shape[1]\n",
    "hidden_sizes = [256, 128,64,1]  \n",
    "dropout = 0  \n",
    "model = Residual_DNN(input_size, hidden_sizes, dropout)\n",
    "\n",
    "# Define the loss function and optimiser\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "# Model Training\n",
    "epochs =30\n",
    "best_mse = float('inf')\n",
    "early_stopping_patience =10\n",
    "best_model_state = None\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate model performance\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_pred = model(inputs_valid)\n",
    "        mse = mean_squared_error(y_valid, valid_pred)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Validation MSE: {mse:.4f}\")\n",
    "        \n",
    "        \n",
    "        scheduler.step(mse)\n",
    "        \n",
    "        # Check and save the best model\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > early_stopping_patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "#Calculate the MSE and MAE on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = model(inputs_test)\n",
    "    test_mse = mean_squared_error(y_test, test_pred)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_pred)\n",
    "    \n",
    "    print(f\"Test MSE with best model: {test_mse:.4f}\")\n",
    "    print(f\"Test MAPE with best model: {test_mape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128d9ac-1e4f-4883-bd3e-7abaacea9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 40% training set\n",
    "inputs_train2 = torch.tensor(inputs_train2).float()\n",
    "y_train2 = torch.tensor(y_train2).float()\n",
    "inputs_valid = torch.tensor(inputs_valid).float()\n",
    "y_valid = torch.tensor(y_valid).float()\n",
    "inputs_test = torch.tensor(inputs_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "train_data = TensorDataset(inputs_train2, y_train2)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=1024, shuffle=True)\n",
    "\n",
    "\n",
    "input_size = inputs_train.shape[1]\n",
    "hidden_sizes = [256, 128,64,1]  \n",
    "dropout = 0  \n",
    "model = Residual_DNN(input_size, hidden_sizes, dropout)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "\n",
    "epochs =30\n",
    "best_mse = float('inf')\n",
    "early_stopping_patience =10\n",
    "best_model_state = None\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_pred = model(inputs_valid)\n",
    "        mse = mean_squared_error(y_valid, valid_pred)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Validation MSE: {mse:.4f}\")\n",
    "        \n",
    "        \n",
    "        scheduler.step(mse)\n",
    "        \n",
    "        \n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > early_stopping_patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = model(inputs_test)\n",
    "    test_mse = mean_squared_error(y_test, test_pred)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_pred)\n",
    "    \n",
    "    print(f\"Test MSE with best model: {test_mse:.4f}\")\n",
    "    print(f\"Test MAPE with best model: {test_mape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435c191b-af83-4fb1-b98d-aae27efcbdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 60% training set\n",
    "inputs_train3 = torch.tensor(inputs_train3).float()\n",
    "y_train3 = torch.tensor(y_train3).float()\n",
    "inputs_valid = torch.tensor(inputs_valid).float()\n",
    "y_valid = torch.tensor(y_valid).float()\n",
    "inputs_test = torch.tensor(inputs_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "train_data = TensorDataset(inputs_train3, y_train3)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=512, shuffle=True)\n",
    "\n",
    "\n",
    "input_size = inputs_train.shape[1]\n",
    "hidden_sizes = [256, 128 ,64,32,1]  \n",
    "dropout = 0  \n",
    "model = Residual_DNN(input_size, hidden_sizes, dropout)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "\n",
    "epochs =30\n",
    "best_mse = float('inf')\n",
    "early_stopping_patience =10\n",
    "best_model_state = None\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_pred = model(inputs_valid)\n",
    "        mse = mean_squared_error(y_valid, valid_pred)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Validation MSE: {mse:.4f}\")\n",
    "        \n",
    "        \n",
    "        scheduler.step(mse)\n",
    "        \n",
    "        \n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > early_stopping_patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = model(inputs_test)\n",
    "    test_mse = mean_squared_error(y_test, test_pred)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_pred)\n",
    "    \n",
    "    print(f\"Test MSE with best model: {test_mse:.4f}\")\n",
    "    print(f\"Test MAPE with best model: {test_mape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe084ce0-0fd7-4083-85b4-39b7b33f80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 80% training set\n",
    "inputs_train4 = torch.tensor(inputs_train4).float()\n",
    "y_train4 = torch.tensor(y_train4).float()\n",
    "inputs_valid = torch.tensor(inputs_valid).float()\n",
    "y_valid = torch.tensor(y_valid).float()\n",
    "inputs_test = torch.tensor(inputs_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "train_data = TensorDataset(inputs_train4, y_train4)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=1024, shuffle=True)\n",
    "\n",
    "\n",
    "input_size = inputs_train.shape[1]\n",
    "hidden_sizes = [256, 128,64,32,16,1]  \n",
    "dropout = 0  \n",
    "model = Residual_DNN(input_size, hidden_sizes, dropout)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "#\n",
    "epochs =40\n",
    "best_mse = float('inf')\n",
    "early_stopping_patience =10\n",
    "best_model_state = None\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_pred = model(inputs_valid)\n",
    "        mse = mean_squared_error(y_valid, valid_pred)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Validation MSE: {mse:.4f}\")\n",
    "        \n",
    "        scheduler.step(mse)\n",
    "        \n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > early_stopping_patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = model(inputs_test)\n",
    "    test_mse = mean_squared_error(y_test, test_pred)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_pred)\n",
    "    \n",
    "    print(f\"Test MSE with best model: {test_mse:.4f}\")\n",
    "    print(f\"Test MAPE with best model: {test_mape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d5d9b-e8e1-4b6f-aa2c-48ce02e2e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# All training data\n",
    "inputs_train = torch.tensor(inputs_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "inputs_valid = torch.tensor(inputs_valid).float()\n",
    "y_valid = torch.tensor(y_valid).float()\n",
    "inputs_test = torch.tensor(inputs_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "train_data = TensorDataset(inputs_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=2048, shuffle=True)\n",
    "\n",
    "\n",
    "input_size = inputs_train.shape[1]\n",
    "hidden_sizes = [256, 128,64,1]  \n",
    "dropout = 0  \n",
    "model = Residual_DNN(input_size, hidden_sizes, dropout)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.08)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.3, verbose=True)\n",
    "\n",
    "\n",
    "epochs =30\n",
    "best_mse = float('inf')\n",
    "early_stopping_patience =10\n",
    "best_model_state = None\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_pred = model(inputs_valid)\n",
    "        mse = mean_squared_error(y_valid, valid_pred)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Validation MSE: {mse:.4f}\")\n",
    "        \n",
    "        \n",
    "        scheduler.step(mse)\n",
    "        \n",
    "        \n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > early_stopping_patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = model(inputs_test)\n",
    "    test_mse = mean_squared_error(y_test, test_pred)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, test_pred)\n",
    "    \n",
    "    print(f\"Test MSE with best model: {test_mse:.4f}\")\n",
    "    print(f\"Test MAPE with best model: {test_mape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b78c607-6627-40c9-8bfd-7f467ca732fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
